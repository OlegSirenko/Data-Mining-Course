

<a name="br2"></a>**Оглавление**

[ВВЕДЕНИЕ...............................................................................................3](#br3)

[ГЛАВА](#br4)[ ](#br4)[1](#br4)[ ](#br4)[ОПИСАНИЕ](#br4)[ ](#br4)[АЛГОРИТМА](#br4)[ ](#br4)[SLIQ.........................................4](#br4)

[ ](#br4)[1.1](#br4)[ ](#br4)[Обзор](#br4)[ ](#br4)[проблемы](#br4)[ ](#br4)[классификации](#br4)[ ](#br4)[в](#br4)[ ](#br4)[области](#br4)[ ](#br4)[интеллектуального
](#br4)[анализа](#br4)[ ](#br4)[данных](#br4)[ ](#br4)[...............................................................................................4](#br4)

[1.2](#br4)[ ](#br4)[Основные](#br4)[ ](#br4)[характеристики](#br4)[ ](#br4)[алгоритма](#br4)[ ](#br4)[SLIQ.................................4](#br4)

[1.3](#br6)[ ](#br6)[Преимущества](#br6)[ ](#br6)[и](#br6)[ ](#br6)[недостатки](#br6)[ ](#br6)[алгоритма](#br6)[ ](#br6)[SLIQ..............................6](#br6)

[ГЛАВА](#br8)[ ](#br8)[2](#br8)[ ](#br8)[РЕАЛИЗАЦИЯ](#br8)[ ](#br8)[АЛГОРИТМА](#br8)[ ](#br8)[SLIQ.....................................8](#br8)

[2.1](#br8)[ ](#br8)[Описание](#br8)[ ](#br8)[ключевых](#br8)[ ](#br8)[компонентов](#br8)[ ](#br8)[алгоритма..............................8](#br8)

[2.2](#br9)[ ](#br9)[Реализации](#br9)[ ](#br9)[на](#br9)[ ](#br9)[языке](#br9)[ ](#br9)[Python](#br9)[ ](#br9)[..........................................................9](#br9)

[ЗАКЛЮЧЕНИЕ.......................................................................................11](#br11)

[СПИСОК](#br13)[ ](#br13)[ИСПОЛЬЗОВАННЫХ](#br13)[ ](#br13)[ИСТОЧНИКОВ.............................13](#br13)

[ПРИЛОЖЕНИЕ......................................................................................14](#br14)

2




<a name="br3"></a>**ВВЕДЕНИЕ**

` `SLIQ (Supervised Learning In Quest) - это быстрый масштабируемый
алгоритм построения дерева решений, который может быть реализован как
последовательно, так и параллельно . Он рекурсивно разбивает обучающий
набор данных с помощью стратегии жадного поиска в ширину, которая
интегрирована с техникой предварительной сортировки во время фазы
построения дерева [1].

` `Целью данной работы является изучение алгоритма SLIQ и его
применение в области интеллектуального анализа данных.

Задачами данной работы являются:

· Определить основные характеристики алгоритма SLIQ.
· Исследовать преимущества и недостатки алгоритма SLIQ.
· Проанализировать результаты практической работы с
 использованием алгоритма SLIQ.

· Изучить возможности для дальнейшего исследования и развития

алгоритма SLIQ.

` `Для достижения поставленной цели нами были изучены общие
принципы работы алгоритма SLIQ и его применение в области
интеллектуального анализа данных. В работе осуществлен обзор основных
характеристик алгоритма SLIQ, таких как его способность обрабатывать
как числовые, так и категориальные признаки, использование техники
предварительной сортировки в фазе роста дерева и стратегии роста дерева
в ширину для классификации дисковых наборов данных. Реализован код
алгоритма SLIQ и предложены результаты его практической работы.

` `Алгоритм SLIQ (Supervised Learning In Quest) был разработан
командой проекта Quest в IBM в 1996 году для майнинга больших наборов
данных. Он является быстрым масштабируемым алгоритмом построения
дерева решений, который может быть реализован как последовательно, так
и параллельно. Он рекурсивно разбивает обучающий набор данных с
помощью стратегии жадного поиска в ширину, которая интегрирована с
техникой предварительной сортировки во время фазы построения дерева.

3




<a name="br4"></a>**ГЛАВА 1**

**ОПИСАНИЕ АЛГОРИТМА SLIQ**

**1.1 Обзор проблемы классификации в области интеллектуального анализа данных**

` `Классификация является важной проблемой в области
интеллектуального анализа данных[2]. Она заключается в том, чтобы на
основе обучающего набора данных построить модель, которая может
предсказывать класс или категорию новых данных [3]. Это достигается
путем анализа обучающего набора данных и выявления закономерностей
и зависимостей между признаками и классами. Затем эти закономерности
используются для построения модели классификации, которая может
применяться к новым данным для предсказания их класса [3].

` `**1.2 Основные характеристики алгоритма SLIQ
 1.2.1 Способность обрабатывать непрерывные и категориальные
признаки.**

` `Способность обрабатывать непрерывные и категориальные признаки:
SLIQ может работать как с непрерывными, так и с категориальными
признаками. Это означает, что алгоритм может обрабатывать данные с
различными типами признаков, что делает его универсальным и
применимым к широкому спектру задач классификации [1].

` `**1.2.2 Использование техники предварительной сортировки в фазе
роста дерева.**

` `Использование техники предварительной сортировки в фазе роста
дерева является одной из ключевых характеристик алгоритма SLIQ. Эта
техника позволяет алгоритму быстро находить оптимальные точки
разбиения для каждого признака и ускоряет процесс построения дерева
решений.

` `В частности, SLIQ использует новую технику предварительной
сортировки в фазе роста дерева. Эта процедура сортировки интегрирована
со стратегией роста дерева в ширину для классификации дисковых наборов
данных. Это означает, что алгоритм строит дерево решений, начиная с
корня и расширяя его в ширину на каждом уровне. В результате этого

4




<a name="br5"></a>алгоритм может эффективно обрабатывать большие наборы данных и
строить точные деревья решений.

` `Техника предварительной сортировки позволяет алгоритму SLIQ
быстро находить оптимальные точки разбиения для каждого признака. Это
достигается путем сортировки значений признаков в порядке возрастания
и вычисления меры неопределенности для каждого возможного разбиения.
Затем алгоритм выбирает разбиение с наименьшей мерой
неопределенности в качестве оптимального. Одной из мер
неопределенности, используемых в алгоритме SLIQ, является Gini-индекс.

` `Gini-индекс - это мера неопределенности или неоднородности
данных, которая вычисляется как сумма квадратов вероятностей каждого
класса. Например, предположим, что у нас есть набор данных с двумя
классами: класс 1 и класс 2. Пусть p1 - это доля примеров, принадлежащих
классу 1, а p2 - это доля примеров, принадлежащих классу 2. Тогда Gini-
индекс можно вычислить по формуле:

Gini = 1 - p1^2 - p2^2 (1)

` `Эта формула может быть обобщена на случай нескольких классов.
Пусть у нас есть n классов и pi - это доля примеров, принадлежащих i-му
классу. Тогда Gini-индекс можно вычислить по формуле:

Gini=1−∑[i=1, n]pi^2

` `В целом, использование техники предварительной сортировки в фазе
роста дерева позволяет алгоритму SLIQ быстро и эффективно строить
точные деревья решений для больших наборов данных. Это делает его
универсальным и применимым к широкому спектру задач классификации.
[1].

` `**1.2.3 Интеграция со стратегией роста дерева в ширину для
классификации дисковых наборов данных.**

` `Интеграция со стратегией роста дерева в ширину: SLIQ интегрирован
со стратегией роста дерева в ширину для классификации дисковых наборов
данных. Это означает, что алгоритм строит дерево решений, начиная с
корня и расширяя его в ширину на каждом уровне. Это позволяет алгоритму
эффективно обрабатывать большие наборы данных и строить точные
деревья решений.

5




<a name="br6"></a> **1.2.4 Использование принципа минимального описания длины
(MDL) для усечения дерева после его построения.**

` `Использование принципа минимального описания длины (MDL) для
усечения дерева: SLIQ использует принцип минимального описания длины
(MDL) для усечения дерева после его построения. MDL - это недорогая
техника усечения дерева, которая использует наименьшее количество
кодирования для создания деревьев малого размера с помощью техники
снизу вверх. Это позволяет алгоритму избавляться от избыточных ветвей
и улучшать точность классификации.

` `Эти характеристики позволяют алгоритму SLIQ быстро и эффективно
строить точные деревья решений для больших наборов данных.

**1.3 Преимущества и недостатки алгоритма SLIQ 1.3.1 Преимущества**

` `Основным преимуществом алгоритма дерева решений SLIQ является
то, что он производит точные деревья решений, которые значительно
меньше деревьев, созданных с помощью C4.5 и CART. Это означает, что
алгоритм может строить компактные и эффективные модели
классификации для больших наборов данных.

` `Таким образом, алгоритм SLIQ позволяет сократить время обучения
и ускорить процесс классификации новых данных. Кроме того, меньший
размер деревьев решений также упрощает их интерпретацию и анализ, что
может быть полезно для понимания закономерностей в данных и выявления
важных признаков.

` `В целом, преимущества алгоритма SLIQ заключаются в его
способности быстро и эффективно строить точные и компактные деревья
решений для больших наборов данных.

**1.3.2 Недостатки**

` `Один из основных недостатков SLIQ заключается в том, что он
использует структуру данных списка классов, которая находится в памяти,
что накладывает ограничения на память для данных. Это означает, что
алгоритм может не справиться с очень большими наборами данных из-за
ограничений на память.

6




<a name="br7"></a> В отличие от некоторых других алгоритмов построения деревьев
решений, которые могут обрабатывать данные прямо с диска, SLIQ требует
загрузки всего обучающего набора данных в память для построения дерева
решений. Это может привести к проблемам с производительностью и
масштабируемостью при работе с очень большими наборами данных.

` `Тем не менее, несмотря на этот недостаток, алгоритм SLIQ все же
является мощным и эффективным инструментом для построения деревьев
решений для больших наборов данных. Он имеет ряд преимуществ,
которые могут перевесить его недостатки в зависимости от конкретной
задачи классификации

7




<a name="br8"></a>**ГЛАВА 2**

**РЕАЛИЗАЦИЯ АЛГОРИТМА SLIQ**

` `**2.1 Описание ключевых компонентов алгоритма**
 Алгоритм начинает с корневого узла дерева и всего набора данных.
Например, предположим, что у нас есть набор данных с тремя атрибутами
(A, B и C) и двумя классами (1 и 2):

Таблица 1: набор данных с тремя атрибутами (A, B и C) и двумя
 классами (1 и 2)

**A B C Класс**

1 X 5 1

2 Y 6 1

3 X 7 2

4 Z 8 2

` `Для каждого атрибута в данных алгоритм вычисляет Gini-индекс для
всех возможных разделений данных на основе этого атрибута. Например,
для атрибута A возможными разделениями являются A < 1.5, A < 2.5 и A <

3\.5. Для каждого разделения алгоритм вычисляет Gini-индекс:

· Разделение A < 1.5: Gini = 0, так как все примеры попадают в правую

группу и принадлежат разным классам.

· Разделение A < 2.5: Gini = 0, так как все примеры попадают в правую

группу и принадлежат разным классам.

· Разделение A < 3.5: Gini = 0.5 \* (0 + 0) = 0, так как левая группа

содержит примеры одного класса, а правая группа содержит примеры разных классов.

` `Аналогичным образом алгоритм вычисляет Gini-индекс для всех
возможных разделений по атрибутам B и C.

` `Алгоритм выбирает атрибут и разделение, которые минимизируют
Gini-индекс. В нашем примере это может быть любое разделение, так как
все они имеют одинаковый Gini-индекс. Допустим, алгоритм выбирает
разделение A < 3.5.

8




<a name="br9"></a>Данные разделяются на две группы на основе выбранного разделения:

Таблица 2: Первая группа сформированная после выбранного

разбиения

**A B C Класс**

**1 X 5 1**

**2 Y 6 1**

Таблица 3: Вторая группа сформированная после выбранного
 разбиения

**A B C Класс**

**3 X 7 2**

**4 Z 8 2**

` `Процесс повторяется рекурсивно для каждой группы данных, пока не
будет достигнуто критерий остановки (например, все примеры в группе
принадлежат одному классу или достигнуто максимальное количество
уровней в дереве). В нашем случае обе группы содержат примеры одного
класса, поэтому процесс останавливается[6].

**2.2 Реализации на языке Python Набор данных:**

` `Мы используем модифицированный набор данных из репозитория
машинного обучения UCI[4], который имеет 9 атрибутов и двоичный
целевой атрибут. Вы можете найти и загрузить данные под названием
«data\_exercise\_2.csv» с веб-сайта курса (смотрите категорию “другие”). Мы
внесли несколько изменений, чтобы получить следующий формат:

` `Первая строка содержит последовательность объявлений типов
атрибутов формы:

a1 : T1, a2 : T2, . . . , ak : Tk, ak+1 : Tk+1

где ai - идентификатор атрибута i, а Ti - его тип (1 ≤ i ≤ k + 1).

9



<a name="br10"></a>Все атрибуты имеют один из следующих типов:

· “n”: числовое значение

· “c”: категориальный, содержащий как минимум три значения

атрибута

· “b”: двоичный файл со значениями атрибутов “yes” и “no”.

` `В загруженном файле данных data\_exercise\_2.csv» первая строка
выглядит следующим образом:

a: n, b:c, c:c, d:n, e:b, f:b, g:c, h:c, i:b, j:t

` `Последний атрибут ak+1 (в приведенном выше примере: “j”) всегда
является целевым атрибутом и имеет тип атрибута “b”.

Каждая последующая строка описывает пример, например,

24, bb, cc, 3, да, нет, gb, hc, да, да

33, bd, cc, 5, да, нет, gb, hd, да, нет

**Результат:**

` `Результат выполнения алгоритма с предоставленным набором
данных можно увидеть на рисунке:

Рисунок 1 – Результат выполнения алгоритма

Код программы представлен в приложении.

10




<a name="br11"></a>**ЗАКЛЮЧЕНИЕ**

` `В рамках данной работы были достигнуты все поставленные цели и
выполнены все задачи. Мы изучили алгоритм SLIQ и его применение в
области интеллектуального анализа данных. В работе был осуществлен
обзор основных характеристик алгоритма SLIQ, таких как его способность
обрабатывать как числовые, так и категориальные признаки, использование
техники предварительной сортировки в фазе роста дерева и стратегии роста
дерева в ширину для классификации дисковых наборов данных. Также
были рассмотрены преимущества и недостатки алгоритма SLIQ.

` `SLIQ (Supervised Learning In Quest) — это быстрый масштабируемый
алгоритм построения дерева решений, который может быть реализован в
последовательном и параллельном режимах. Он не основан на алгоритме
HUNT для классификации деревьев решений. Он рекурсивно разбивает
обучающий набор данных с помощью стратегии жадного поиска в ширину,
которая интегрирована с техникой предварительной сортировки во время
фазы построения дерева. При построении модели дерева решений SLIQ
обрабатывает как числовые, так и категориальные атрибуты.

` `Одно из основных преимуществ алгоритма дерева решений SLIQ
заключается в том, что он производит точные деревья решений, которые
значительно меньше, чем деревья, созданные с помощью C4.5 и CART. В
то же время SLIQ выполняется почти на порядок быстрее, чем CART1[5].

Основные характеристики алгоритма SLIQ:

· Классификатор деревьев, обрабатывающий как числовые, так и

категориальные атрибуты

· Предварительная сортировка числовых атрибутов до построения

дерева

· Стратегия роста в ширину

· Тест на хорошее состояние - индекс Джини

· Недорогой алгоритм обрезки деревьев на основе минимальной длины

описания (MDL).

` `Во второй главе работы была описана реализация алгоритма SLIQ на
языке Python. Мы описали ключевые компоненты алгоритма и предложили

11




<a name="br12"></a>примеры кода для его реализации. Также были проанализированы
результаты практической работы с использованием алгоритма SLIQ.

` `В целом, результаты нашего исследования показывают, что алгоритм
SLIQ является мощным и эффективным инструментом для классификации
больших наборов данных. Он может быть использован для решения
широкого спектра задач в области интеллектуального анализа данных.

` `Возможности для дальнейшего исследования и развития. Можно
рассмотреть возможность улучшения алгоритма для работы с очень
большими наборами данных, которые не могут быть полностью загружены
в память. Также можно исследовать возможность интеграции алгоритма
SLIQ с другими методами машинного обучения для улучшения точности
классификации.

` `Кроме того, можно рассмотреть возможность разработки новых
методов усечения деревьев решений, которые могут улучшить точность
классификации и уменьшить размер деревьев. Также можно исследовать
возможность использования алгоритма SLIQ для решения других задач
машинного обучения, таких как регрессия или кластеризация.

` `В целом, есть много возможностей для дальнейшего исследования и
развития алгоритма SLIQ, которые могут привести к улучшению его
производительности и расширению области применения.

12




<a name="br13"></a> **СПИСОК ИСПОЛЬЗОВАННЫХ ИСТОЧНИКОВ**

1\. SLIQ: A fast scalable classifier for data mining | SpringerLink.
[ ](https://link.springer.com/chapter/10.1007/BFb0014141)<https://link.springer.com/chapter/10.1007/BFb0014141>

2\. <https://www.upgrad.com/blog/classification-in-data-mining/>

3\. <https://www.geeksforgeeks.org/basic-concept-classification-data-mining/>

4\. <https://archive.ics.uci.edu/ml/datasets/Contraceptive+Method+Choice>

5\. Decision Tree: SPRINT vs SLIQ? - Data Science Stack Exchange.
[ ](https://datascience.stackexchange.com/questions/109450/decision-tree-sprint-vs-sliq)[https://datascience.stackexchange.com/questions/109450/decision-tree-
](https://datascience.stackexchange.com/questions/109450/decision-tree-sprint-vs-sliq)[ ](https://datascience.stackexchange.com/questions/109450/decision-tree-sprint-vs-sliq)[sprint-vs-sliq](https://datascience.stackexchange.com/questions/109450/decision-tree-sprint-vs-sliq).

6\. [https://www.cs.uni-](https://www.cs.uni-potsdam.de/ml/teaching/ws13/ml/Entscheidungsbaume-English-2.pdf)

[potsdam.de/ml/teaching/ws13/ml/Entscheidungsbaume-English-2.pdf](https://www.cs.uni-potsdam.de/ml/teaching/ws13/ml/Entscheidungsbaume-English-2.pdf)

13




<a name="br14"></a>**ПРИЛОЖЕНИЕ**

**Код программы алгоритма SLIQ:**

**import random**

**import sys**

**# split data file into attributes, training data and testing data**

**def datasets(dataset, percent):**

**f = dataset.replace('\r\n','\n').split('\n')**

**attr = {}**

**attr['id'] = [x.split(':')[0] for x in f[0].split(',')]**

**attr['type'] = [x.split(':')[1] for x in f[0].split(',')]**

**attr['length'] = len(attr['id']) - 1**

**# extract numerical data**

**flag = int(percent \* (len(f)-1))**

**data = [x.split(',') for x in f[1:] if len(x)>0]**

**for itemIndex in xrange(len(data)):**

**for attrIndex in xrange(attr['length']):**

**if attr['type'][attrIndex] == 'n':**

**data[itemIndex][attrIndex] = int(data[itemIndex][attrIndex])**

**random.shuffle(data)**

**return attr, data[:flag], data[flag:]**

**# find all subsets of categorical attributes**

**def subset(set):**

**subset = []**

**length = len(set)**

14




<a name="br15"></a>**for iteration in xrange(2\*\*length):**

**curset = []**

**for digit in xrange(length):**

**if iteration % 2:**

**curset.append(set[digit])**

**iteration /= 2**

**subset.append(curset)**

**return subset**

**# presort each attribute list and class index**

**def presort(attr, dataset):**

**attrList = []**

**attr['value'] = {}**

**for attrIndex in xrange(attr['length']):**

**# all attribute combinations**

**if attr['type'][attrIndex] == 'b':**

**attr['value'][attrIndex] = ['yes']**

**elif attr['type'][attrIndex] == 'n':**

**attr['value'][attrIndex] = sorted(list(set([data[attrIndex] for data in dataset])))**

**elif attr['type'][attrIndex] == 'c':**

**attr['value'][attrIndex] = subset(list(set([data[attrIndex] for data in dataset])))**

**# sorted attribute lists**

**currAttrList = []**

**for dataIndex in xrange(len(dataset)):**

**currAttrList.append([dataset[dataIndex][attrIndex], dataIndex])**

**attrList.append(sorted(currAttrList))**

**return attrList, [[data[attr['length']],1] for data in dataset]**

15




<a name="br16"></a>**# return Gini**

**def gini(Ly, Ln, Ry, Rn):**

**L = Ly + Ln**

**R = Ry + Rn**

**if L:**

**GL = (1.0-(float(Ly)/L)\*\*2-(float(Ln)/L)\*\*2)\*L/(L+R)**

**else:**

**GL = 0**

**if R:**

**GR = (1.0-(float(Ry)/R)\*\*2-(float(Rn)/R)\*\*2)\*R/(L+R)**

**else:**

**GR = 0**

**return GL + GR**

**# return test result: yes/no**

**def judge(data, attribute, value):**

**if attribute == 'n':**

**return data < value**

**if attribute == 'c':**

**return data in value**

**if attribute == 'b':**

**return data == 'yes'**

**# test for the node**

**def print\_value(name, attribute, value):**

**if attribute == 'n':**

**return str(name) + ' < ' + str(value)**

**if attribute == 'c':**

**return name + ' in {' + (',').join(value) + '}'**

16



<a name="br17"></a>**if attribute == 'b':**

**return name**

**# print histogram for each node during middle steps**

**def print\_histogram(node, value, countLy, countLn, countRy, countRn):**

**print 'N'+str(node)+' on '+value**

**print '\t|yes\t|no'**

**print 'L\t|'+str(countLy)+'\t|'+str(countLn)**

**print 'R\t|'+str(countRy)+'\t|'+str(countRn)**

**print**

**# print class index for each node during middle steps**

**def print\_class(classList, node):**

**print 'N'+str(node)+':'**

**for index in xrange(len(classList)):**

**if classList[index][1] == node:**

**print str(index)+'\t|'+classList[index][0]**

**print**

**# detect whether a node is a leaf or not**

**def is\_leaf(classList, no):**

**node = list(set([x[0] for x in classList if x[1]==no]))**

**if len(node) != 1:**

**return "split"**

**else:**

**return node[0] # yes/no**

**# evaluate and split by all attribute and gini funciton**

**def evaluate\_split(attr, attrList, classList, node, middlestep, used):**

17




<a name="br18"></a>**dataNum= len(classList)**

**minGini = 1**

**minIndex = -1**

**minValue = 0**

**deadlock = False**

**for attrIndex in xrange(attr['length']):**

**if attrIndex in used:**

**continue**

**for value in attr['value'][attrIndex]:**

**# update histogram**

**countLy = 0**

**countLn = 0**

**countRy = 0**

**countRn = 0**

**# read attrList**

**for item in xrange(dataNum):**

**if classList[attrList[attrIndex][item][1]][1] != node:**

**continue**

**if judge(attrList[attrIndex][item][0], attr['type'][attrIndex], value):**

**if classList[attrList[attrIndex][item][1]][0] == 'yes':**

||**else:**||<p>**countLy += 1**</p><p>**countLn += 1**</p>|
| :- | :- | :- | :- |
**else:**

**if classList[attrList[attrIndex][item][1]][0] == 'yes':**

||**else:**||<p>**countRy += 1**</p><p>**countRn += 1**</p>|
| :- | :- | :- | :- |
18




<a name="br19"></a>**if middlestep:**

` `**print\_histogram(node,
print\_value(attr['id'][attrIndex], attr['type'][attrIndex], value), countLy, countLn,
countRy, countRn)**

**if gini(countLy, countLn, countRy, countRn) < minGini:**

**minGini = gini(countLy, countLn, countRy, countRn)**

**minValue = value**

**minIndex = attrIndex**

` `**# deadlocks stands for those samples are the same
attribute values but contradict in classification**

**deadlock = ((countLy\*countLn>0) and (countRy+countRn)==0) or ((countRy\*countRn>0) and (countLy+countLn)==0)**

**if deadlock:**

**if countLy + countRy > countLn + countRn:**

**deadlock = 'yes'**

**else:**

**deadlock = 'no'**

**# free attrList**

**return minIndex, minValue, deadlock**

**# generate decision tree**

**def generate\_tree(attr, attrList, classList, middlestep):**

**tree = {}**

**tree[1] = 'yes'**

**queue = [[1]]**

**dataNum= len(classList)**

**# breath first search**

**while queue:**

**tnode= queue.pop(0)**

**node = tnode[0]**

**used = tnode[1:]**

19




<a name="br20"></a>**if middlestep:**

**print\_class(classList, node)**

**# leaf terminates the split**

**flag = is\_leaf(classList, node)**

**if flag != "split":**

**tree[node] = flag**

**continue**

**# evaluate split**

` `**minIndex, minValue, deadlock = evaluate\_split(attr, attrList,
classList, node, middlestep, used)**

**if minIndex == -1:**

**continue**

**# deadlock terminates the plit**

**if deadlock:**

**tree[node] = deadlock**

**continue**

**left = len(tree) + 1**

**right = left + 1**

` `**tree[node] = [tree[node], print\_value(attr['id'][minIndex],
attr['type'][minIndex], minValue), left, right, minIndex, attr['type'][minIndex],
minValue]**

**tree[left] = 'yes'**

**tree[right] = 'no'**

**# update node class id**

**for item in xrange(dataNum):**

**if classList[attrList[minIndex][item][1]][1] != node:**

**continue**

**if judge(attrList[minIndex][item][0], attr['type'][minIndex], minValue):**

**classList[attrList[minIndex][item][1]][1] = left**

**else:**

20




<a name="br21"></a>**classList[attrList[minIndex][item][1]][1] = right**

**# add left and right node to queue**

**qleft = [left]**

**qright = [right]**

**for item in used:**

**qleft.append(item)**

**qright.append(item)**

**if minIndex not in used:**

**qleft.append(minIndex)**

**qright.append(minIndex)**

**queue.append(qleft)**

**queue.append(qright)**

**return tree**

**# print decision tree**

**def print\_tree(tree, no, deep, output):**

**if tree[no] == 'yes' or tree[no] == 'no':**

**return**

**#print '-'\*deep,**

**left = tree[no][2]**

**right = tree[no][3]**

**if tree[left] == 'yes' or tree[left] == 'no':**

**left = tree[left]**

**if tree[right] == 'yes' or tree[right] == 'no':**

**right = tree[right]**

**node = '%d %s %s %s %s' %(no, tree[no][0], tree[no][1], str(left), str(right))**

**print node**

**output.write(node+'\n')**

**print\_tree(tree, tree[no][2], deep+1, output)**

21




<a name="br22"></a>**print\_tree(tree, tree[no][3], deep+1, output)**

**# find leaf of decision tree recursively**

**def test\_node(tree, node, data):**

**if tree[node] == 'yes':**

**return 'yes'**

**if tree[node] == 'no':**

**return 'no'**

**if judge(data[tree[node][4]], tree[node][5], tree[node][6]):**

**return test\_node(tree, tree[node][2], data)**

**else:**

**return test\_node(tree, tree[node][3], data)**

**# test decision tree**

**def test\_tree(tree, testData):**

**length = len(testData)**

**if not length:**

**return 'Null'**

**count = 0**

**for data in testData:**

**if test\_node(tree, 1, data) == data[-1]:**

**count += 1**

**return float(count)/length**

**# SLIQ step by step**

**def train(fname, percent=2.0/3, middlestep=0):**

**with open(fname) as f:**

**file = f.read()**

**attr, trainData, testData = datasets(file, percent)**

22



<a name="br23"></a>**attrList, classList = presort(attr, trainData)**

**tree = generate\_tree(attr, attrList, classList, middlestep)**

**print 'SLIQ:'**

**output = open('result.txt','w')**

**print\_tree(tree, 1, 0, output)**

**output.close()**

**print**

**print 'Train Data Precision: %.4f' %test\_tree(tree, trainData)**

**print 'Test Data Precision: %.4f' %test\_tree(tree, testData)**

**return tree**

**if \_\_name\_\_ == '\_\_main\_\_':**

**# params: file, training percentage, show middlesteps**

**if len(sys.argv) < 2:**

**train('data\_exercise\_2.csv', 2.0/3, 1)**

**else:**

**train(sys.argv[1])**

23
